{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random forest classifier from our prexiesting code is pretty simple, we just need to select a random subset of the features at every split.\n",
    "\n",
    "We can do that with this code snippet before we split our trees\n",
    "```python\n",
    "numFeatures = int(np.round_(np.sqrt(X.shape[1])))\n",
    "X = X.sample(n = numFeatures, axis=\"columns\")\n",
    "```\n",
    "\n",
    "beyond that, we just need to use the same general bagging strategy we did on lab 5. That is, create many different trees with resampling and then taking the mode of their predictions.\n",
    "\n",
    "But first we need to set up the project and import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "home = str(Path.home()) # all other paths are relative to this path. change to something else if this is not the case on your system\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# make sure your run the cell above before running this\n",
    "\n",
    "import Lab4_helper\n",
    "import Lab5_helper\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the diabetes dataset from lab 4\n",
    "\n",
    "loading the data in and grabbing the features we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "diabetes_df = pd.read_csv(\n",
    "    f\"./diabetes_indicators.csv\"\n",
    ")\n",
    "features = ['Sex','Age','Education','Income','Fruits','Veggies','Smoker', \"HighChol\", \"BMI\"]\n",
    "dia_X = diabetes_df.loc[:,features][:1000]\n",
    "dia_X = dia_X.dropna()\n",
    "dia_t = diabetes_df.loc[dia_X.index,'Diabetes_012']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying our input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sex   Age  Education  Income  Fruits  Veggies  Smoker  HighChol   BMI\n",
       "0    0.0   9.0        4.0     3.0     0.0      1.0     1.0       1.0  40.0\n",
       "1    0.0   7.0        6.0     1.0     0.0      0.0     1.0       0.0  25.0\n",
       "2    0.0   9.0        4.0     8.0     1.0      0.0     0.0       1.0  28.0\n",
       "3    0.0  11.0        3.0     6.0     1.0      1.0     0.0       0.0  27.0\n",
       "4    0.0  11.0        5.0     4.0     1.0      1.0     0.0       1.0  24.0\n",
       "..   ...   ...        ...     ...     ...      ...     ...       ...   ...\n",
       "995  0.0   2.0        6.0     8.0     1.0      0.0     0.0       0.0  31.0\n",
       "996  0.0  10.0        5.0     8.0     0.0      1.0     0.0       0.0  21.0\n",
       "997  1.0   7.0        4.0     1.0     0.0      0.0     0.0       1.0  31.0\n",
       "998  0.0   5.0        4.0     8.0     1.0      1.0     0.0       0.0  37.0\n",
       "999  1.0  11.0        4.0     7.0     0.0      1.0     1.0       0.0  28.0\n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dia_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying our target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.0\n",
       "1      0.0\n",
       "2      0.0\n",
       "3      0.0\n",
       "4      0.0\n",
       "      ... \n",
       "995    0.0\n",
       "996    0.0\n",
       "997    0.0\n",
       "998    0.0\n",
       "999    0.0\n",
       "Name: Diabetes_012, Length: 1000, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dia_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the f1 scores for various algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'myrf_classifier': 0.8569341877380964,\n",
       " 'bagging_classifier': 0.7700010085851262,\n",
       " 'skrf_classifier': 0.7977920649619576,\n",
       " 'boost_classifier': 0.8203250785647629}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_results = {}\n",
    "helper.run_myrf(X=dia_X, t= dia_t, results=classification_results, type=\"classifier\",ntrials=10, ntrees=25)\n",
    "helper.run_bagging(X=dia_X, t= dia_t, results=classification_results, type=\"classifier\",ntrials=10, ntrees=25)\n",
    "helper.run_skrf(X=dia_X, t= dia_t, results=classification_results, type=\"classifier\",ntrials=10, ntrees=25)\n",
    "helper.run_boost(X=dia_X, t= dia_t, results=classification_results, type=\"classifier\",ntrials=10, ntrees=25)\n",
    "classification_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really cool! Our random forest preformed better than sklearn's random forest, as well as the bagging and boosting algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only major changes we need to make to create a random forest is how we split our trees, our base case in regression, and how we make our predictions.\n",
    "### Splitting Trees\n",
    "In the classification case, our goal was to minimize entropy. In the regression case, our goal is to minimize variance. In the code, this was simply changing their criterion in the regressive case to the variance.\n",
    "```python\n",
    "def gain(y, x, type=\"classifier\"):\n",
    "    g = 0\n",
    "    possibleValues = x.unique()\n",
    "    weightedCriterions = []\n",
    "\n",
    "    for value in possibleValues:\n",
    "        #splitting the data by values\n",
    "        xAtVal = x.loc[x == value]\n",
    "        yAtVal = y.loc[x == value]\n",
    "        #calculating our gain\n",
    "        if type==\"classifier\":\n",
    "            unweightedCriterion = entropy(yAtVal)\n",
    "        elif type==\"regressor\":\n",
    "            unweightedCriterion = yAtVal.var()\n",
    "        #weighting it\n",
    "        weight = xAtVal.size / x.size\n",
    "        weightedCriterions.append(weight * unweightedCriterion)\n",
    "        \n",
    "    #seeing how much we improved\n",
    "    g = sum(weightedCriterions)\n",
    "    if type == \"classifier\":\n",
    "        origCriterion = entropy(y)\n",
    "    if type == \"regressor\":\n",
    "        origCriterion = y.var()\n",
    "\n",
    "    return origCriterion - g\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Base Case and Predictions\n",
    "When we're constructing a tree and we reach a point to make a node, such as when we run out of feature or when we go under our minimum samples need to split, we needed to change the code to be the average of the remaining targets instead of the mode. Similarly when we're predicting with all of our classifiers, we need  to take the prediction on average instead of the mode of the predictions for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use a dataset that contains the chemical properties of various red wines and their numerical rating of \"quality\" by experts. I'm going to try to do a regression between those chemical properties and the quality. I obtained the dataset from https://archive.ics.uci.edu/ml/datasets/wine+quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset in and preprocessing\n",
    "displaying the raw dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./winequality-red.csv\", delimiter=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "displaying the preprocessed X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.528194</td>\n",
       "      <td>0.961576</td>\n",
       "      <td>-1.391037</td>\n",
       "      <td>-0.453077</td>\n",
       "      <td>-0.243630</td>\n",
       "      <td>-0.466047</td>\n",
       "      <td>-0.379014</td>\n",
       "      <td>0.558100</td>\n",
       "      <td>1.288240</td>\n",
       "      <td>-0.579025</td>\n",
       "      <td>-0.959946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.298454</td>\n",
       "      <td>1.966827</td>\n",
       "      <td>-1.391037</td>\n",
       "      <td>0.043403</td>\n",
       "      <td>0.223805</td>\n",
       "      <td>0.872365</td>\n",
       "      <td>0.624168</td>\n",
       "      <td>0.028252</td>\n",
       "      <td>-0.719708</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>-0.584594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.298454</td>\n",
       "      <td>1.296660</td>\n",
       "      <td>-1.185699</td>\n",
       "      <td>-0.169374</td>\n",
       "      <td>0.096323</td>\n",
       "      <td>-0.083643</td>\n",
       "      <td>0.228975</td>\n",
       "      <td>0.134222</td>\n",
       "      <td>-0.331073</td>\n",
       "      <td>-0.048074</td>\n",
       "      <td>-0.584594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.654339</td>\n",
       "      <td>-1.384011</td>\n",
       "      <td>1.483689</td>\n",
       "      <td>-0.453077</td>\n",
       "      <td>-0.264878</td>\n",
       "      <td>0.107558</td>\n",
       "      <td>0.411372</td>\n",
       "      <td>0.664069</td>\n",
       "      <td>-0.978798</td>\n",
       "      <td>-0.461036</td>\n",
       "      <td>-0.584594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.528194</td>\n",
       "      <td>0.961576</td>\n",
       "      <td>-1.391037</td>\n",
       "      <td>-0.453077</td>\n",
       "      <td>-0.243630</td>\n",
       "      <td>-0.466047</td>\n",
       "      <td>-0.379014</td>\n",
       "      <td>0.558100</td>\n",
       "      <td>1.288240</td>\n",
       "      <td>-0.579025</td>\n",
       "      <td>-0.959946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0      -0.528194          0.961576    -1.391037       -0.453077  -0.243630   \n",
       "1      -0.298454          1.966827    -1.391037        0.043403   0.223805   \n",
       "2      -0.298454          1.296660    -1.185699       -0.169374   0.096323   \n",
       "3       1.654339         -1.384011     1.483689       -0.453077  -0.264878   \n",
       "4      -0.528194          0.961576    -1.391037       -0.453077  -0.243630   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n",
       "0            -0.466047             -0.379014  0.558100  1.288240  -0.579025   \n",
       "1             0.872365              0.624168  0.028252 -0.719708   0.128910   \n",
       "2            -0.083643              0.228975  0.134222 -0.331073  -0.048074   \n",
       "3             0.107558              0.411372  0.664069 -0.978798  -0.461036   \n",
       "4            -0.466047             -0.379014  0.558100  1.288240  -0.579025   \n",
       "\n",
       "    alcohol  \n",
       "0 -0.959946  \n",
       "1 -0.584594  \n",
       "2 -0.584594  \n",
       "3 -0.584594  \n",
       "4 -0.959946  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_X = df.drop(columns=[\"quality\"])\n",
    "wine_t = df[\"quality\"]\n",
    "wine_X = wine_X.apply(lambda col: (col - col.mean()) / col.std())\n",
    "wine_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying random forest and calculating the RMSE for different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'myrf_regressor': 0.7060103298790785,\n",
       " 'boost_regressor': 0.7291675413269797,\n",
       " 'skrf_regressor': 0.5934734308985723,\n",
       " 'bagging_regressor': 0.5986632100773788}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_results = {}\n",
    "ntrials = 5\n",
    "ntrees = 25\n",
    "helper.run_myrf(X=wine_X, t=wine_t, results=regression_results, type=\"regressor\", ntrials=ntrials, ntrees=ntrees)\n",
    "helper.run_boost(X=wine_X, t=wine_t, results=regression_results, type=\"regressor\", ntrials=ntrials, ntrees=ntrees)\n",
    "helper.run_skrf(X=wine_X, t=wine_t, results=regression_results, type=\"regressor\", ntrials=ntrials, ntrees=ntrees)\n",
    "helper.run_bagging(X=wine_X, t=wine_t, results=regression_results, type=\"regressor\", ntrials=ntrials, ntrees=ntrees)\n",
    "regression_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is disapointing that our implementation of random forest performed worse than sklearn's, but since it preformed better  than boosting, I don't think there's any errors with it -- it's clearly doing something right. If I had to guess it's because we're selecting the split in continuous data differently than sklearn does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "We're going to measure the feature importance by seeing how the feature, on average across all the trees, increases the purity in the resulting dataset. This increase in purity is mathematically defined by a decrease in the gini inpurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Gain at a Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity\n",
    "The Gini Impurity score at node $n$ is defined as the probability of picking two different classes if you picked randomly from all the samples at node n. It ranges from 0 to 1 and the higher it is, the more impure the data and the lower it is, the more pure the data. It's mathematically described as$$g(n) = 1- \\sum_{i=1}^{j}(P_i)^2$$\n",
    "where:\n",
    "\n",
    "$n$ is the current node (split in the dataset based on the value of some feature)\n",
    "\n",
    "$j$ is the number of distinct classes,\n",
    "\n",
    "$P_i$ is what portion the $i$-th class is of all the classes (the probaiblity of selecting the $i$-th class at random)\n",
    "\n",
    "\n",
    "We can implement it with this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(x):\n",
    "    counts = x.value_counts()\n",
    "    fracs = counts / len(x)\n",
    "    ans = 1 - (fracs ** 2).sum()\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with a more impure dataset A and a more pure dataset B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impurity  of A is 0.7901234567901234\n",
      "Impurity of B is 0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "A = pd.Series([1,2, 2, 3, 4, 4, 1, 5, 5])\n",
    "B = pd.Series([0, 1, 1, 1, 1, 0, 1, 1, 0])\n",
    "\n",
    "print(\"Impurity  of A is\", gini(A))\n",
    "print(\"Impurity of B is\", gini(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Gini Impurity Gain\n",
    "We're interested in average decrease in gini impurity at the current node -- aka average increase in purity. That is, the difference between the gini impurity at $n$ and the weighted sum of the gini impurity of its two children. The weight of each child is what proportion of the samples at $n$ are included in the child. Mathematically this is\n",
    "$$gg(n) = g(n) - \\sum_{i = 1}^{c} \\frac{s_i}{s_n}g(c_i)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$n$ is the current node\n",
    "\n",
    "$gg(n)$ is the mean gini impurity decrease at $n$\n",
    "\n",
    "$c$ is the number of children\n",
    "\n",
    "$s_i$ is the number of samples included in the $i$-th child\n",
    "\n",
    "$s_n$ is the number of samples at $n$\n",
    "\n",
    "$g(c_i)$ is the gini impurity of the $i$-th child node\n",
    "\n",
    "we can implement it with this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, t, tree):\n",
    "    #helper function to split the data by the feature given in the tree\n",
    "    feature_name, threshold = list(tree.keys())[0].split(\"<\")\n",
    "    threshold = float(threshold)\n",
    "\n",
    "    #Split the data\n",
    "    x_l = x[x[feature_name] < threshold]\n",
    "    x_r = x[x[feature_name] >= threshold]\n",
    "    t_l = t[x[feature_name] < threshold]\n",
    "    t_r = t[x[feature_name] >= threshold]\n",
    "\n",
    "    return x_l, x_r, t_l, t_r\n",
    "\n",
    "def gid(x, t, tree):\n",
    "    #split the data by the metric in the tree. The node n is the head node of the tree. \n",
    "    #Grab the metric in question\n",
    "    x_l, x_r, t_l, t_r  = split_data(x, t, tree)\n",
    "    \n",
    "    #calculate gid\n",
    "    p_l = len(x_l) / len(x)\n",
    "    p_r = len(x_r) / len(x)\n",
    "\n",
    "    #calculating ginis\n",
    "    gini_n = gini(t)\n",
    "    gini_l = gini(t_l)\n",
    "    gini_r = gini(t_r)\n",
    "\n",
    "    ans = gini_n - (p_l * gini_l + p_r * gini_r)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function on a table that can be neatly divided in two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"isHot\": [0, 0, 0, 1, 1, 1], \"shouldTouch\": [1, 1, 1, 0, 0, 0,]}\n",
    "test_df = pd.DataFrame(data)\n",
    "tree = {\n",
    "    \"isHot<0.5\": {\"False\": 1,\n",
    "                  \"True\": 0}\n",
    "}\n",
    "test_x = test_df.drop(columns=[\"shouldTouch\"])\n",
    "test_t = test_df.drop(columns=[\"isHot\"])\n",
    "gid(test_x, test_t, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense! As the impurity at the start is 0.5, and the purity of each of the split tables is 0, so the weighted decrease in impurity is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on Lab 4 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015036706311076897"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gid(dia_X_test, dia_t_test, c45_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does that seem reasonable? I have no idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance from Gini Impurity Decrease\n",
    "For every tree, for every node, we find the Gini impurity decrease and then weight it by the proportion of the  number of samples at the node to the number of samples in total. We take that value and average it over each feature. This is our feature importance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement this like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for recursing through tree and calculating gid at every node:\n",
    "def gini_importance_from_tree(x, t, tree, n_samples, feature_results):\n",
    "    #defining base cases\n",
    "    if len(x) == 0:\n",
    "        return \n",
    "    if not isinstance(tree, dict):\n",
    "        return\n",
    "    feature_name, threshold = list(tree.keys())[0].split(\"<\")\n",
    "    #getting the purity increase and weighting it\n",
    "    gid_i  = gid(x, t, tree)\n",
    "    importance = gid_i * (len(x) / n_samples)\n",
    "    #adding it to the  results\n",
    "    if feature_name in feature_results:\n",
    "        feature_results[feature_name].append(importance)\n",
    "    else:\n",
    "        feature_results[feature_name] = list([importance])\n",
    "\n",
    "    #recursing\n",
    "    subtree = list(tree.values())[0]\n",
    "    for expected_value, next_tree in subtree.items():\n",
    "        sub_x = x[(x[feature_name] < float(threshold)) == (expected_value == \"True\")]\n",
    "        sub_t = t[(x[feature_name] < float(threshold)) == (expected_value == \"True\")]\n",
    "        gini_importance_from_tree(sub_x, sub_t, next_tree, n_samples, feature_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the feature importance metric on the diabetes dataset and displaying the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age          0.010144\n",
       "BMI          0.007309\n",
       "Income       0.005576\n",
       "HighChol     0.004016\n",
       "Education    0.001489\n",
       "Veggies      0.001478\n",
       "Fruits       0.000559\n",
       "Sex          0.000383\n",
       "Smoker       0.000283\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing\n",
    "from sklearn.model_selection import train_test_split\n",
    "#setting up \n",
    "ntrials = 5\n",
    "ntrees = 25\n",
    "default = 0\n",
    "feature_results = {}\n",
    "#running tests\n",
    "for trial in range(ntrials):\n",
    "    X_train, X_test, t_train, t_test = train_test_split(dia_X, dia_t, test_size=0.3,random_state=trial)\n",
    "    trees = helper.make_rf_trees(X_train,t_train,ntrees=ntrees)\n",
    "    for tree in trees:\n",
    "        gini_importance_from_tree(X_train, t_train, tree, len(X_train), feature_results)\n",
    "\n",
    "data = {key: sum(value) / len(value) for key, value in feature_results.items()}\n",
    "pd.Series(data).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense! This metrics intuitively seem important for estimating BMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the feature importances for sklearn's implementation of random forest usign their built-in feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BMI          0.274652\n",
       "Age          0.207724\n",
       "Income       0.169856\n",
       "Education    0.105071\n",
       "Fruits       0.054698\n",
       "HighChol     0.049250\n",
       "Sex          0.046786\n",
       "Veggies      0.046446\n",
       "Smoker       0.045517\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ntrials = 5\n",
    "ntrees = 25\n",
    "default = 0\n",
    "sk_feature_results = {}\n",
    "for trial in range(ntrials):\n",
    "    X_train, X_test, t_train, t_test = train_test_split(dia_X, dia_t, test_size=0.3,random_state=trial)\n",
    "    classifier = RandomForestClassifier(n_estimators=ntrees, random_state=trial, min_samples_split=5, criterion=\"entropy\").fit(X_train,t_train)\n",
    "    for i in range(len(classifier.feature_names_in_)):\n",
    "        feature = classifier.feature_names_in_[i]\n",
    "        importance = classifier.feature_importances_[i]\n",
    "        if feature not in sk_feature_results:\n",
    "            sk_feature_results[feature] = [importance]\n",
    "        else:\n",
    "            sk_feature_results[feature].append(importance)\n",
    "\n",
    "pd.DataFrame(sk_feature_results).T.mean(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's more is that it has very similar results to the built-in implementation! The top 3 are all the same and education is in the top 5  of both. I'm willing to chalk the remaining differences up to how we split continuous data again. My model and sklearn's model did have different F1 score, so the models are different and it shouldn't be too surprising that the importances then differ also."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('Main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1b7983f5379a74aaa065db4f4f1794df776760a88f2be34c33f726ce7ee9d88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
